<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 <HEAD>
   <TITLE> [antlr-interest] Bounding the token stream in the C backend
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:antlr-interest%40antlr.org?Subject=Re:%20%5Bantlr-interest%5D%20Bounding%20the%20token%20stream%20in%20the%20C%20backend&In-Reply-To=%3C4a051d931003021844o6da3fc49q1e01a247e9bd478e%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="037839.html">
   <LINK REL="Next"  HREF="037841.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[antlr-interest] Bounding the token stream in the C backend</H1>
    <B>Christopher L Conway</B> 
    <A HREF="mailto:antlr-interest%40antlr.org?Subject=Re:%20%5Bantlr-interest%5D%20Bounding%20the%20token%20stream%20in%20the%20C%20backend&In-Reply-To=%3C4a051d931003021844o6da3fc49q1e01a247e9bd478e%40mail.gmail.com%3E"
       TITLE="[antlr-interest] Bounding the token stream in the C backend">cconway at cs.nyu.edu
       </A><BR>
    <I>Tue Mar  2 18:44:13 PST 2010</I>
    <P><UL>
        <LI>Previous message: <A HREF="037839.html">[antlr-interest] Out of memory problem for filter grammar
</A></li>
        <LI>Next message: <A HREF="037841.html">[antlr-interest] Bounding the token stream in the C backend
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#37840">[ date ]</a>
              <a href="thread.html#37840">[ thread ]</a>
              <a href="subject.html#37840">[ subject ]</a>
              <a href="author.html#37840">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On Thu, Feb 25, 2010 at 12:09 AM, Christopher L Conway
&lt;<A HREF="http://www.antlr.org/mailman/listinfo/antlr-interest">cconway at cs.nyu.edu</A>&gt; wrote:
&gt;<i> I've got a large input file (~39MB) that I'm attempting to parse with
</I>&gt;<i> an ANTLR3-generated C parser. The parser is using a huge amount of
</I>&gt;<i> memory (~3.7GB) and seems to start thrashing without making much
</I>&gt;<i> progress towards termination. I found a thread from earlier this month
</I>&gt;<i> (<A HREF="http://markmail.org/message/jfngdd2ci6h7qrbo">http://markmail.org/message/jfngdd2ci6h7qrbo</A>) suggesting the most
</I>&gt;<i> likely cause of such behavior is a parser bug, but I've stepped
</I>&gt;<i> through the code and it seems to be lexing just fine. Rather, it seems
</I>&gt;<i> the problem is that fillBuffer() is tokenizing the whole file in one
</I>&gt;<i> go; then, the parsing rules slow to a crawl because the token buffer
</I>&gt;<i> is sitting on all the memory.
</I>&gt;<i>
</I>&gt;<i> I wonder if there is a way to change fillBuffer()'s behavior, so that
</I>&gt;<i> it will only lex some bounded number of tokens before allowing parsing
</I>&gt;<i> to proceed?
</I>
I have a partial solution to this problem. To be clear, the issue is:

1. The default token stream implementation tokenizes the entire input
on the first call to LT().
2. The default token factory never de-allocates tokens.

Since a token structure is more than 100 bytes, large inputs can
easily consume multiple GB before parsing even begins. (This is in the
C back-end. I don't know about other back-ends.)

The solution consists of:

1. A token stream implementation that tokenizes up to a fixed lookahead limit.
2. A token factory that pre-allocates a fixed number of tokens,
recycling old tokens when new ones are requested.

This seems to be a sound strategy, so long as the input grammar has an
known lookahead limit and the allocation pool is sufficiently large.
My grammar is LL(2), and a lookahead limit of 2 with a token pool of 8
tokens works just fine.

Using this implementation, I'm able to parse the above-mentioned 39MB
input file using less than 700MB memory, a more than 5-fold
improvement on the defaults (as an added benefit, the parser actually
terminated after 45s and didn't completely lock my system!). The
parser is about 40% faster than an equivalent ANTLR 2.7 parser using
the C++ back-end, but still uses about 5 times as much memory. The
remaining excess memory usage seems to be due to the default string
factory implementation, which also doesn't seem to ever release memory
once allocated. This is a much more complex beast and I'm hesitant to
tackle it.

If anybody is interested in using this code, I'm willing to clean it
up and share it with the community. Please feel free to contact me.

Regards,
Chris
</PRE>



<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="037839.html">[antlr-interest] Out of memory problem for filter grammar
</A></li>
	<LI>Next message: <A HREF="037841.html">[antlr-interest] Bounding the token stream in the C backend
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#37840">[ date ]</a>
              <a href="thread.html#37840">[ thread ]</a>
              <a href="subject.html#37840">[ subject ]</a>
              <a href="author.html#37840">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://www.antlr.org/mailman/listinfo/antlr-interest">More information about the antlr-interest
mailing list</a><br>
</body></html>
